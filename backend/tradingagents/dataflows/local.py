from __future__ import annotations

from typing import List, Dict, Optional, Iterable, Tuple, Annotated, Any
import pandas as pd
import os, time, json, requests, re, asyncio
from .config import DATA_DIR
from datetime import datetime
from dateutil.relativedelta import relativedelta
from .reddit_utils import fetch_top_from_category
from tqdm import tqdm
from tradingview_ta import TA_Handler, Interval
from datetime import datetime, timezone
import math
import requests


def get_YFin_data_window(
    symbol: Annotated[str, "ticker symbol of the company"],
    curr_date: Annotated[str, "Start date in yyyy-mm-dd format"],
    look_back_days: Annotated[int, "how many days to look back"],
) -> str:
    # calculate past days
    date_obj = datetime.strptime(curr_date, "%Y-%m-%d")
    before = date_obj - relativedelta(days=look_back_days)
    start_date = before.strftime("%Y-%m-%d")

    # read in data
    data = pd.read_csv(
        os.path.join(
            DATA_DIR,
            f"market_data/price_data/{symbol}-YFin-data-2015-01-01-2025-03-25.csv",
        )
    )

    # Extract just the date part for comparison
    data["DateOnly"] = data["Date"].str[:10]

    # Filter data between the start and end dates (inclusive)
    filtered_data = data[
        (data["DateOnly"] >= start_date) & (data["DateOnly"] <= curr_date)
    ]

    # Drop the temporary column we created
    filtered_data = filtered_data.drop("DateOnly", axis=1)

    # Set pandas display options to show the full DataFrame
    with pd.option_context(
        "display.max_rows", None, "display.max_columns", None, "display.width", None
    ):
        df_string = filtered_data.to_string()

    return (
        f"## Raw Market Data for {symbol} from {start_date} to {curr_date}:\n\n"
        + df_string
    )

def get_YFin_data(
    symbol: Annotated[str, "ticker symbol of the company"],
    start_date: Annotated[str, "Start date in yyyy-mm-dd format"],
    end_date: Annotated[str, "End date in yyyy-mm-dd format"],
) -> str:
    # read in data
    data = pd.read_csv(
        os.path.join(
            DATA_DIR,
            f"market_data/price_data/{symbol}-YFin-data-2015-01-01-2025-03-25.csv",
        )
    )

    if end_date > "2025-03-25":
        raise Exception(
            f"Get_YFin_Data: {end_date} is outside of the data range of 2015-01-01 to 2025-03-25"
        )

    # Extract just the date part for comparison
    data["DateOnly"] = data["Date"].str[:10]

    # Filter data between the start and end dates (inclusive)
    filtered_data = data[
        (data["DateOnly"] >= start_date) & (data["DateOnly"] <= end_date)
    ]

    # Drop the temporary column we created
    filtered_data = filtered_data.drop("DateOnly", axis=1)

    # remove the index from the dataframe
    filtered_data = filtered_data.reset_index(drop=True)

    return filtered_data

def get_finnhub_news(
    query: Annotated[str, "Search query or ticker symbol"],
    start_date: Annotated[str, "Start date in yyyy-mm-dd format"],
    end_date: Annotated[str, "End date in yyyy-mm-dd format"],
):
    """
    Retrieve news about a company within a time frame

    Args
        query (str): Search query or ticker symbol
        start_date (str): Start date in yyyy-mm-dd format
        end_date (str): End date in yyyy-mm-dd format
    Returns
        str: dataframe containing the news of the company in the time frame

    """

    result = get_data_in_range(query, start_date, end_date, "news_data", DATA_DIR)

    if len(result) == 0:
        return ""

    combined_result = ""
    for day, data in result.items():
        if len(data) == 0:
            continue
        for entry in data:
            current_news = (
                "### " + entry["headline"] + f" ({day})" + "\n" + entry["summary"]
            )
            combined_result += current_news + "\n\n"

    return f"## {query} News, from {start_date} to {end_date}:\n" + str(combined_result)


def get_finnhub_company_insider_sentiment(
    ticker: Annotated[str, "ticker symbol for the company"],
    curr_date: Annotated[str, "current date you are trading at, yyyy-mm-dd"],
):
    """
    Retrieve insider sentiment about a company (retrieved from public SEC information) for the past 15 days
    Args:
        ticker (str): ticker symbol of the company
        curr_date (str): current date you are trading on, yyyy-mm-dd
    Returns:
        str: a report of the sentiment in the past 15 days starting at curr_date
    """

    date_obj = datetime.strptime(curr_date, "%Y-%m-%d")
    before = date_obj - relativedelta(days=15)  # Default 15 days lookback
    before = before.strftime("%Y-%m-%d")

    data = get_data_in_range(ticker, before, curr_date, "insider_senti", DATA_DIR)

    if len(data) == 0:
        return ""

    result_str = ""
    seen_dicts = []
    for date, senti_list in data.items():
        for entry in senti_list:
            if entry not in seen_dicts:
                result_str += f"### {entry['year']}-{entry['month']}:\nChange: {entry['change']}\nMonthly Share Purchase Ratio: {entry['mspr']}\n\n"
                seen_dicts.append(entry)

    return (
        f"## {ticker} Insider Sentiment Data for {before} to {curr_date}:\n"
        + result_str
        + "The change field refers to the net buying/selling from all insiders' transactions. The mspr field refers to monthly share purchase ratio."
    )


def get_finnhub_company_insider_transactions(
    ticker: Annotated[str, "ticker symbol"],
    curr_date: Annotated[str, "current date you are trading at, yyyy-mm-dd"],
):
    """
    Retrieve insider transcaction information about a company (retrieved from public SEC information) for the past 15 days
    Args:
        ticker (str): ticker symbol of the company
        curr_date (str): current date you are trading at, yyyy-mm-dd
    Returns:
        str: a report of the company's insider transaction/trading informtaion in the past 15 days
    """

    date_obj = datetime.strptime(curr_date, "%Y-%m-%d")
    before = date_obj - relativedelta(days=15)  # Default 15 days lookback
    before = before.strftime("%Y-%m-%d")

    data = get_data_in_range(ticker, before, curr_date, "insider_trans", DATA_DIR)

    if len(data) == 0:
        return ""

    result_str = ""

    seen_dicts = []
    for date, senti_list in data.items():
        for entry in senti_list:
            if entry not in seen_dicts:
                result_str += f"### Filing Date: {entry['filingDate']}, {entry['name']}:\nChange:{entry['change']}\nShares: {entry['share']}\nTransaction Price: {entry['transactionPrice']}\nTransaction Code: {entry['transactionCode']}\n\n"
                seen_dicts.append(entry)

    return (
        f"## {ticker} insider transactions from {before} to {curr_date}:\n"
        + result_str
        + "The change field reflects the variation in share count‚Äîhere a negative number indicates a reduction in holdings‚Äîwhile share specifies the total number of shares involved. The transactionPrice denotes the per-share price at which the trade was executed, and transactionDate marks when the transaction occurred. The name field identifies the insider making the trade, and transactionCode (e.g., S for sale) clarifies the nature of the transaction. FilingDate records when the transaction was officially reported, and the unique id links to the specific SEC filing, as indicated by the source. Additionally, the symbol ties the transaction to a particular company, isDerivative flags whether the trade involves derivative securities, and currency notes the currency context of the transaction."
    )

def get_data_in_range(ticker, start_date, end_date, data_type, data_dir, period=None):
    """
    Gets finnhub data saved and processed on disk.
    Args:
        start_date (str): Start date in YYYY-MM-DD format.
        end_date (str): End date in YYYY-MM-DD format.
        data_type (str): Type of data from finnhub to fetch. Can be insider_trans, SEC_filings, news_data, insider_senti, or fin_as_reported.
        data_dir (str): Directory where the data is saved.
        period (str): Default to none, if there is a period specified, should be annual or quarterly.
    """

    if period:
        data_path = os.path.join(
            data_dir,
            "finnhub_data",
            data_type,
            f"{ticker}_{period}_data_formatted.json",
        )
    else:
        data_path = os.path.join(
            data_dir, "finnhub_data", data_type, f"{ticker}_data_formatted.json"
        )

    data = open(data_path, "r")
    data = json.load(data)

    # filter keys (date, str in format YYYY-MM-DD) by the date range (str, str in format YYYY-MM-DD)
    filtered_data = {}
    for key, value in data.items():
        if start_date <= key <= end_date and len(value) > 0:
            filtered_data[key] = value
    return filtered_data

def get_simfin_balance_sheet(
    ticker: Annotated[str, "ticker symbol"],
    freq: Annotated[
        str,
        "reporting frequency of the company's financial history: annual / quarterly",
    ],
    curr_date: Annotated[str, "current date you are trading at, yyyy-mm-dd"],
):
    data_path = os.path.join(
        DATA_DIR,
        "fundamental_data",
        "simfin_data_all",
        "balance_sheet",
        "companies",
        "us",
        f"us-balance-{freq}.csv",
    )
    df = pd.read_csv(data_path, sep=";")

    # Convert date strings to datetime objects and remove any time components
    df["Report Date"] = pd.to_datetime(df["Report Date"], utc=True).dt.normalize()
    df["Publish Date"] = pd.to_datetime(df["Publish Date"], utc=True).dt.normalize()

    # Convert the current date to datetime and normalize
    curr_date_dt = pd.to_datetime(curr_date, utc=True).normalize()

    # Filter the DataFrame for the given ticker and for reports that were published on or before the current date
    filtered_df = df[(df["Ticker"] == ticker) & (df["Publish Date"] <= curr_date_dt)]

    # Check if there are any available reports; if not, return a notification
    if filtered_df.empty:
        print("No balance sheet available before the given current date.")
        return ""

    # Get the most recent balance sheet by selecting the row with the latest Publish Date
    latest_balance_sheet = filtered_df.loc[filtered_df["Publish Date"].idxmax()]

    # drop the SimFinID column
    latest_balance_sheet = latest_balance_sheet.drop("SimFinId")

    return (
        f"## {freq} balance sheet for {ticker} released on {str(latest_balance_sheet['Publish Date'])[0:10]}: \n"
        + str(latest_balance_sheet)
        + "\n\nThis includes metadata like reporting dates and currency, share details, and a breakdown of assets, liabilities, and equity. Assets are grouped as current (liquid items like cash and receivables) and noncurrent (long-term investments and property). Liabilities are split between short-term obligations and long-term debts, while equity reflects shareholder funds such as paid-in capital and retained earnings. Together, these components ensure that total assets equal the sum of liabilities and equity."
    )


def get_simfin_cashflow(
    ticker: Annotated[str, "ticker symbol"],
    freq: Annotated[
        str,
        "reporting frequency of the company's financial history: annual / quarterly",
    ],
    curr_date: Annotated[str, "current date you are trading at, yyyy-mm-dd"],
):
    data_path = os.path.join(
        DATA_DIR,
        "fundamental_data",
        "simfin_data_all",
        "cash_flow",
        "companies",
        "us",
        f"us-cashflow-{freq}.csv",
    )
    df = pd.read_csv(data_path, sep=";")

    # Convert date strings to datetime objects and remove any time components
    df["Report Date"] = pd.to_datetime(df["Report Date"], utc=True).dt.normalize()
    df["Publish Date"] = pd.to_datetime(df["Publish Date"], utc=True).dt.normalize()

    # Convert the current date to datetime and normalize
    curr_date_dt = pd.to_datetime(curr_date, utc=True).normalize()

    # Filter the DataFrame for the given ticker and for reports that were published on or before the current date
    filtered_df = df[(df["Ticker"] == ticker) & (df["Publish Date"] <= curr_date_dt)]

    # Check if there are any available reports; if not, return a notification
    if filtered_df.empty:
        print("No cash flow statement available before the given current date.")
        return ""

    # Get the most recent cash flow statement by selecting the row with the latest Publish Date
    latest_cash_flow = filtered_df.loc[filtered_df["Publish Date"].idxmax()]

    # drop the SimFinID column
    latest_cash_flow = latest_cash_flow.drop("SimFinId")

    return (
        f"## {freq} cash flow statement for {ticker} released on {str(latest_cash_flow['Publish Date'])[0:10]}: \n"
        + str(latest_cash_flow)
        + "\n\nThis includes metadata like reporting dates and currency, share details, and a breakdown of cash movements. Operating activities show cash generated from core business operations, including net income adjustments for non-cash items and working capital changes. Investing activities cover asset acquisitions/disposals and investments. Financing activities include debt transactions, equity issuances/repurchases, and dividend payments. The net change in cash represents the overall increase or decrease in the company's cash position during the reporting period."
    )


def get_simfin_income_statements(
    ticker: Annotated[str, "ticker symbol"],
    freq: Annotated[
        str,
        "reporting frequency of the company's financial history: annual / quarterly",
    ],
    curr_date: Annotated[str, "current date you are trading at, yyyy-mm-dd"],
):
    data_path = os.path.join(
        DATA_DIR,
        "fundamental_data",
        "simfin_data_all",
        "income_statements",
        "companies",
        "us",
        f"us-income-{freq}.csv",
    )
    df = pd.read_csv(data_path, sep=";")

    # Convert date strings to datetime objects and remove any time components
    df["Report Date"] = pd.to_datetime(df["Report Date"], utc=True).dt.normalize()
    df["Publish Date"] = pd.to_datetime(df["Publish Date"], utc=True).dt.normalize()

    # Convert the current date to datetime and normalize
    curr_date_dt = pd.to_datetime(curr_date, utc=True).normalize()

    # Filter the DataFrame for the given ticker and for reports that were published on or before the current date
    filtered_df = df[(df["Ticker"] == ticker) & (df["Publish Date"] <= curr_date_dt)]

    # Check if there are any available reports; if not, return a notification
    if filtered_df.empty:
        print("No income statement available before the given current date.")
        return ""

    # Get the most recent income statement by selecting the row with the latest Publish Date
    latest_income = filtered_df.loc[filtered_df["Publish Date"].idxmax()]

    # drop the SimFinID column
    latest_income = latest_income.drop("SimFinId")

    return (
        f"## {freq} income statement for {ticker} released on {str(latest_income['Publish Date'])[0:10]}: \n"
        + str(latest_income)
        + "\n\nThis includes metadata like reporting dates and currency, share details, and a comprehensive breakdown of the company's financial performance. Starting with Revenue, it shows Cost of Revenue and resulting Gross Profit. Operating Expenses are detailed, including SG&A, R&D, and Depreciation. The statement then shows Operating Income, followed by non-operating items and Interest Expense, leading to Pretax Income. After accounting for Income Tax and any Extraordinary items, it concludes with Net Income, representing the company's bottom-line profit or loss for the period."
    )

def get_reddit_global_news(
    curr_date: Annotated[str, "Current date in yyyy-mm-dd format"],
    look_back_days: Annotated[int, "Number of days to look back"] = 7,
    limit: Annotated[int, "Maximum number of articles to return"] = 5,
) -> str:
    """
    Retrieve the latest top reddit news
    Args:
        curr_date: Current date in yyyy-mm-dd format
        look_back_days: Number of days to look back (default 7)
        limit: Maximum number of articles to return (default 5)
    Returns:
        str: A formatted string containing the latest news articles posts on reddit
    """

    curr_date_dt = datetime.strptime(curr_date, "%Y-%m-%d")
    before = curr_date_dt - relativedelta(days=look_back_days)
    before = before.strftime("%Y-%m-%d")

    posts = []
    # iterate from before to curr_date
    curr_iter_date = datetime.strptime(before, "%Y-%m-%d")

    total_iterations = (curr_date_dt - curr_iter_date).days + 1
    pbar = tqdm(desc=f"Getting Global News on {curr_date}", total=total_iterations)

    while curr_iter_date <= curr_date_dt:
        curr_date_str = curr_iter_date.strftime("%Y-%m-%d")
        fetch_result = fetch_top_from_category(
            "global_news",
            curr_date_str,
            limit,
            data_path=os.path.join(DATA_DIR, "reddit_data"),
        )
        posts.extend(fetch_result)
        curr_iter_date += relativedelta(days=1)
        pbar.update(1)

    pbar.close()

    if len(posts) == 0:
        return ""

    news_str = ""
    for post in posts:
        if post["content"] == "":
            news_str += f"### {post['title']}\n\n"
        else:
            news_str += f"### {post['title']}\n\n{post['content']}\n\n"

    return f"## Global News Reddit, from {before} to {curr_date}:\n{news_str}"

def get_reddit_companynews(
    query: Annotated[str, "Search query or ticker symbol"],
    start_date: Annotated[str, "Start date in yyyy-mm-dd format"],
    end_date: Annotated[str, "End date in yyyy-mm-dd format"],
) -> str:
    """
    Retrieve the latest top reddit news
    Args:
        query: Search query or ticker symbol
        start_date: Start date in yyyy-mm-dd format
        end_date: End date in yyyy-mm-dd format
    Returns:
        str: A formatted string containing news articles posts on reddit
    """

    start_date_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_date_dt = datetime.strptime(end_date, "%Y-%m-%d")

    posts = []
    # iterate from start_date to end_date
    curr_date = start_date_dt

    total_iterations = (end_date_dt - curr_date).days + 1
    pbar = tqdm(
        desc=f"Getting Company News for {query} from {start_date} to {end_date}",
        total=total_iterations,
    )

    while curr_date <= end_date_dt:
        curr_date_str = curr_date.strftime("%Y-%m-%d")
        fetch_result = fetch_top_from_category(
            "company_news",
            curr_date_str,
            10,  # max limit per day
            query,
            data_path=os.path.join(DATA_DIR, "reddit_data"),
        )
        posts.extend(fetch_result)
        curr_date += relativedelta(days=1)

        pbar.update(1)

    pbar.close()

    if len(posts) == 0:
        return ""

    news_str = ""
    for post in posts:
        if post["content"] == "":
            news_str += f"### {post['title']}\n\n"
        else:
            news_str += f"### {post['title']}\n\n{post['content']}\n\n"

    return f"##{query} News Reddit, from {start_date} to {end_date}:\n\n{news_str}"

# ------------------------------ EDIT NEWS(GLOBAL) -----------------------------------------#

# ---------------- Finnhub News  -----------------
DEFAULT_API_KEY = os.getenv("FINNHUB_API_KEY")

def _fh_client(api_key: Optional[str] = None) -> finnhub.Client:
    key = api_key or DEFAULT_API_KEY
    if not key or key == "YOUR_API_KEY_HERE":
        raise RuntimeError("‚ùå Missing FINNHUB_API_KEY. Please set env variable or pass it explicitly.")
    return finnhub.Client(api_key=key)

# ===================== I/O HELPERS =====================
def _ensure_dir(path: str):
    if os.path.dirname(path):
        os.makedirs(os.path.dirname(path), exist_ok=True)

def save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for obj in items:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    print(f"üíæ Saved {len(items)} items to {path}")

# ===================== DATA HELPERS =====================
def _to_iso_or_raw(ts):
    if isinstance(ts, (int, float)):
        return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()
    return ts

WANTED_KEYS = {"datetime", "headline", "id", "source", "summary", "url"}

def project_fields(items: List[Dict]) -> List[Dict]:
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç + ‡πÅ‡∏õ‡∏•‡∏á datetime ‡πÄ‡∏õ‡πá‡∏ô ISO 8601"""
    out = []
    for it in items:
        obj = {k: it.get(k) for k in WANTED_KEYS}
        obj["datetime"] = _to_iso_or_raw(obj.get("datetime"))
        out.append(obj)
    return out

# ===================== CORE LOGIC =====================
def fetch_finnhub_world_news_raw(
    category: str = "general",
    *,
    limit: int = 50,         # <--- ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô limit ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    sleep_s: float = 0.6,
    api_key: Optional[str] = None
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß '‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ/‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏•‡∏Å' ‡∏à‡∏≤‡∏Å Finnhub ‡πÅ‡∏ö‡∏ö RAW (‡πÑ‡∏°‡πà‡∏ï‡∏±‡∏î‡∏ü‡∏¥‡∏•‡∏î‡πå)
    - ‡πÉ‡∏ä‡πâ Logic ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô limit
    """
    client = _fh_client(api_key)
    all_items: List[Dict] = []
    min_id = 0
    
    print(f"üîÑ Fetching '{category}' news (Limit: {limit})...")

    while len(all_items) < limit:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API (min_id=0 ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏≠‡∏≤‡∏Ç‡πà‡∏≤‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
        try:
            batch = client.general_news(category, min_id=min_id)
        except Exception as e:
            print(f"‚ö†Ô∏è API Error: {e}")
            break

        if not batch:
            break
            
        all_items.extend(batch)
        print(f"   ... Got {len(batch)} items. Total so far: {len(all_items)}")
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß break ‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á sleep
        if len(all_items) >= limit:
            break

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° min_id ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (‡πÄ‡∏≠‡∏≤ id ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ)
        min_id = batch[-1].get("id", min_id)
        time.sleep(sleep_s) # ‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏ô‡πÅ‡∏ö‡∏ô

    return all_items[:limit] # ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡∏ó‡∏¥‡πâ‡∏á

def fetch_finnhub_world_news(
    limit: int = 50,
    save_jsonl_path: Optional[str] = "data/global_news/finnhub_world_news.jsonl",
    return_raw: bool = False,
    api_key: Optional[str] = None
) -> List[Dict]:
    """
    Function ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å
    """
    # 1. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Raw
    raw_items = fetch_finnhub_world_news_raw(
        category="general", 
        limit=limit, 
        api_key=api_key
    )

    # 2. ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Raw ‡πÉ‡∏´‡πâ Return ‡πÄ‡∏•‡∏¢ (‡πÅ‡∏•‡∏∞ Save ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ path)
    if return_raw:
        if save_jsonl_path:
            save_jsonl(raw_items, save_jsonl_path, append=False)
        return raw_items

    # 3. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Slim format (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ)
    slim_items = project_fields(raw_items)

    # 4. Save ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ path)
    if save_jsonl_path:
        save_jsonl(slim_items, save_jsonl_path, append=False)

    return slim_items
    
# ------------------------------ EDIT NEWS GLOBAL(REDDIT) -----------------------------------------#
    
from typing import Iterable, List, Dict, Optional
import praw

DEFAULT_SUBS = ["worldnews", "news", "business", "economics"]

# ---------------------------
# Client / IO Utilities
# ---------------------------

def make_reddit_client(
    client_id: Optional[str] = None,
    client_secret: Optional[str] = None,
    user_agent: Optional[str] = None,
    ratelimit_seconds: int = 5,
) -> praw.Reddit:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á PRAW client ‡πÅ‡∏ö‡∏ö read-only
    ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å ENV:
      REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT
    """
    client_id = os.getenv("REDDIT_ID")
    client_secret = os.getenv("REDDIT_SECRET")
    user_agent = os.getenv("REDDIT_USER_AGENT")

    if not client_id or not user_agent:
        raise RuntimeError("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á ENV/‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå: REDDIT_CLIENT_ID ‡πÅ‡∏•‡∏∞ REDDIT_USER_AGENT (REDDIT_CLIENT_SECRET ‡πÉ‡∏™‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πá‡πÑ‡∏î‡πâ)")

    reddit = praw.Reddit(
        client_id=client_id,
        client_secret=client_secret or None,  # ‡∏ö‡∏≤‡∏á‡πÅ‡∏≠‡∏õ‡πÅ‡∏ö‡∏ö installed ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ß‡πâ‡∏ô‡πÑ‡∏î‡πâ
        user_agent=user_agent,
        ratelimit_seconds=ratelimit_seconds,
    )
    reddit.read_only = True
    return reddit

def _ts_to_iso(ts: float | int | None) -> Optional[str]:
    if ts is None:
        return None
    try:
        return datetime.fromtimestamp(float(ts), tz=timezone.utc).isoformat()
    except Exception:
        return None


def _normalize_post(p) -> Dict:
    """‡∏î‡∏∂‡∏á‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç + ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πá‡∏ô ISO"""
    return {
        "id": p.id,
        "subreddit": str(getattr(p, "subreddit", "")),
        "title": p.title,
        "url": p.url,
        "permalink": f"https://reddit.com{p.permalink}",
        "flair": getattr(p, "link_flair_text", None),
        "created_utc": float(getattr(p, "created_utc", 0.0)) if getattr(p, "created_utc", None) is not None else None,
        "created_iso": _ts_to_iso(getattr(p, "created_utc", None)),
        "selftext": (getattr(p, "selftext", "") or "").strip(),
        "score": int(getattr(p, "score", 0)),
        "num_comments": int(getattr(p, "num_comments", 0)),
        "over_18": bool(getattr(p, "over_18", False)),
        "author": str(getattr(getattr(p, "author", None), "name", "") or ""),
    }


def fetch_subreddit_top(
    reddit: praw.Reddit,
    sub: str,
    *,
    time_filter: str = "day",   # "hour" | "day" | "week" | "month" | "year" | "all"
    limit: int = 20,
    skip_nsfw: bool = True,
    sleep_per_item: float = 0.05,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå Top ‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏á subreddit ‡∏´‡∏ô‡∏∂‡πà‡∏á
    ‡∏Ñ‡∏∑‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å normalize ‡πÅ‡∏•‡πâ‡∏ß (dict)
    """
    out: List[Dict] = []
    try:
        for p in reddit.subreddit(sub).top(time_filter=time_filter, limit=limit):
            if skip_nsfw and bool(getattr(p, "over_18", False)):
                continue
            out.append(_normalize_post(p))
            if sleep_per_item:
                time.sleep(sleep_per_item)  # ‡∏ñ‡∏µ‡πà‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏≠‡∏≤‡∏à‡πÇ‡∏î‡∏ô 429
    except Exception as e:
        print(f"[WARN] r/{sub} error: {e}")
    return out


def fetch_multi_subs_top(
    reddit: praw.Reddit,
    subs: Iterable[str] = DEFAULT_SUBS,
    *,
    time_filter: str = "day",
    per_sub_limit: int = 20,
    dedupe: bool = True,
    sort_by_score_desc: bool = True,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏≤‡∏¢ subreddit ‡πÅ‡∏•‡πâ‡∏ß (option) ‡∏•‡∏ö‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ id ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° score
    """
    all_posts: List[Dict] = []
    seen: set[str] = set()

    for s in subs:
        batch = fetch_subreddit_top(
            reddit, s, time_filter=time_filter, limit=per_sub_limit
        )
        if not dedupe:
            all_posts.extend(batch)
            continue

        for it in batch:
            pid = it.get("id")
            if not pid or pid in seen:
                continue
            seen.add(pid)
            all_posts.append(it)

    if sort_by_score_desc:
        all_posts.sort(key=lambda x: (x.get("score") or 0), reverse=True)

    return all_posts


def fetch_world_news_today(
    subs: Iterable[str] = DEFAULT_SUBS,
    *,
    per_sub_limit: int = 20,
    time_filter: str = "day",
    client: Optional[praw.Reddit] = None,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á Top ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏á‡∏≠‡∏∑‡πà‡∏ô‡∏ï‡∏≤‡∏° time_filter) ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ subs ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    """
    reddit = client or make_reddit_client()
    return fetch_multi_subs_top(
        reddit,
        subs=subs,
        time_filter=time_filter,
        per_sub_limit=per_sub_limit,
        dedupe=True,
        sort_by_score_desc=True,
    )


def fetch_reddit_world_news() -> str:
    """
    ‡∏î‡∏∂‡∏á ‚Üí ‡∏£‡∏ß‡∏° ‚Üí ‡πÄ‡∏ã‡∏ü‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå (JSONL ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)
    ‡∏Ñ‡∏∑‡∏ô path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    """
    out_path: str = "data/global_news/reddit_world_news.jsonl"
    subs: Iterable[str] = DEFAULT_SUBS
    per_sub_limit: int = 20
    time_filter: str = "week"
    jsonl: bool = True
    posts = fetch_world_news_today(
        subs=subs,
        per_sub_limit=per_sub_limit,
        time_filter=time_filter,
    )
    if jsonl:
        save_jsonl(posts, out_path, append=False)
    else:
        # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå
        save_json(posts, out_path)
    return out_path

# ------------------------------ EDIT NEWS GLOBAL(YFINANCE) -----------------------------------------#

from datetime import datetime, timedelta, timezone
from yfinance import Search

def _to_epoch(dt: datetime) -> int:
    return int(dt.replace(tzinfo=timezone.utc).timestamp())

def _ensure_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def _save_json(items: List[Dict], path: str):
    _ensure_dir(path)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def _save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for obj in items:
            f.write(json.dumps(obj, ensure_ascii=False))
            f.write("\n")

def _window_epochs(curr_date: str, look_back_days: int) -> Tuple[int, int]:
    """‡∏Ñ‡∏∑‡∏ô (start_epoch, end_epoch) ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏£‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤"""
    curr_dt  = datetime.strptime(curr_date, "%Y-%m-%d")
    start_dt = (curr_dt - timedelta(days=look_back_days)).replace(hour=0, minute=0, second=0, microsecond=0)
    end_dt   = curr_dt.replace(hour=23, minute=59, second=59, microsecond=0)
    return _to_epoch(start_dt), _to_epoch(end_dt)

_YF_KEEP = {
    "title", "link", "publisher", "providerPublishTime",
    "type", "uuid", "relatedTickers", "thumbnail"
}

def _normalize_item(n: Dict) -> Dict:
    """‡∏Ñ‡∏á‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç + ‡πÄ‡∏û‡∏¥‡πà‡∏° iso & date_str ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô/‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏∞‡∏î‡∏ß‡∏Å"""
    out = {k: n.get(k) for k in _YF_KEEP}
    ts = n.get("providerPublishTime")
    if isinstance(ts, (int, float)):
        dt = datetime.fromtimestamp(int(ts), tz=timezone.utc)
        out["published_iso"] = dt.isoformat()
        out["published_date"] = dt.strftime("%Y-%m-%d")
    else:
        out["published_iso"] = None
        out["published_date"] = None
    # ‡∏ö‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ uuid ‚Üí ‡πÉ‡∏ä‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ä‡πà‡∏ß‡∏¢ dedupe
    out["_dedup_key"] = str(out.get("uuid") or out.get("link") or "")
    return out


# ---------------------------
# Fetchers
# ---------------------------

def search_yf_news(keyword: str, *, news_count: int = 40) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å yfinance.Search ‡∏ï‡πà‡∏≠‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    ‡∏Ñ‡∏∑‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡∏¥‡∏ö (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà normalize/‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤)
    """
    try:
        s = Search(keyword, news_count=news_count)
        return list(getattr(s, "news", []) or [])
    except Exception:
        return []

def fetch_yf_news_by_keywords(
    keywords: Iterable[str],
    *,
    start_epoch: Optional[int] = None,
    end_epoch: Optional[int] = None,
    per_keyword: int = 40,
    limit_total: int = 50,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î ‚Üí ‡∏£‡∏ß‡∏° ‚Üí ‡∏•‡∏ö‡∏ã‡πâ‡∏≥ ‚Üí ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô ‚Üí ‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏° limit_total
    ‡∏Ñ‡∏∑‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å normalize ‡πÅ‡∏•‡πâ‡∏ß
    """
    seen: set[str] = set()
    kept: List[Dict] = []

    for kw in keywords:
        raw = search_yf_news(kw, news_count=per_keyword)
        for n in raw:
            item = _normalize_item(n)
            dedup = item.get("_dedup_key", "")
            if not dedup or dedup in seen:
                continue
            ts = n.get("providerPublishTime")
            if isinstance(ts, (int, float)) and start_epoch is not None and end_epoch is not None:
                if not (start_epoch <= int(ts) <= end_epoch):
                    continue
            elif not isinstance(ts, (int, float)):
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
                continue
            seen.add(dedup)
            kept.append(item)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
    kept.sort(key=lambda x: x.get("providerPublishTime", 0), reverse=True)
    return kept[:limit_total]


# ---------------------------
# Orchestrator (World News)
# ---------------------------

DEFAULT_WORLD_KEYWORDS = [
    "world", "world news", "geopolitics", "global economy", "international"
]

def get_world_news_yf() -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á '‡∏Ç‡πà‡∏≤‡∏ß‡πÇ‡∏•‡∏Å' ‡∏î‡πâ‡∏ß‡∏¢ yfinance (‡∏Ñ‡πâ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ):
      1) ‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤ [curr_date - look_back_days, curr_date]
      2) ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î
      3) ‡∏£‡∏ß‡∏°/‡∏•‡∏ö‡∏ã‡πâ‡∏≥/‡∏Å‡∏£‡∏≠‡∏á‡∏ß‡∏±‡∏ô/‡πÄ‡∏£‡∏µ‡∏¢‡∏á ‚Üí ‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏° limit
      4) (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡πÄ‡∏ã‡∏ü JSON/JSONL

    return: list[dict] ‡∏ó‡∏µ‡πà normalize ‡πÅ‡∏•‡πâ‡∏ß
    """
    curr_date: str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    look_back_days: int = 7
    limit: int = 50
    keywords: Optional[Iterable[str]] = None
    per_keyword: Optional[int] = None
    save_jsonl_path: Optional[str] = "data/global_news/yfinance_world_news.jsonl"
    start_epoch, end_epoch = _window_epochs(curr_date, look_back_days)
    kw = list(keywords) if keywords else DEFAULT_WORLD_KEYWORDS

    # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤
    start_epoch, end_epoch = _window_epochs(curr_date, look_back_days)

    # 2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏ß‡∏¥‡∏£‡πå‡∏î
    kw = list(keywords) if keywords else DEFAULT_WORLD_KEYWORDS

    # 3) ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏µ‡∏î‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å
    if per_keyword is None:
        per_keyword = max(10, min(100, (limit // max(1, len(kw))) * 2 or 20))

    # 4) ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å yfinance (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏¥‡∏ö‡∏à‡∏∞‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡πÄ‡∏ä‡πà‡∏ô: uuid, title, link, publisher, providerPublishTime)
    raw_items = fetch_yf_news_by_keywords(
        kw,
        start_epoch=start_epoch,
        end_epoch=end_epoch,
        per_keyword=per_keyword,
        limit_total=limit,
    )

    # 5) map ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 5 ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    normalized: List[Dict] = []
    seen = set()
    for n in raw_items:
        # ‡πÉ‡∏ä‡πâ uuid ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏±‡∏Å; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ fallback ‡πÄ‡∏õ‡πá‡∏ô link
        dedup = n.get("uuid") or n.get("link")
        if not dedup or dedup in seen:
            continue
        seen.add(dedup)

        normalized.append({
            "uuid":           n.get("uuid") or "",
            "publisher":      n.get("publisher") or "",
            "title":          n.get("title") or "",
            "link":           n.get("link") or "",
            "published_date": _epoch_to_iso(n.get("providerPublishTime")),
        })

    # 6) (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSONL
    if save_jsonl_path:
        os.makedirs(os.path.dirname(save_jsonl_path) or ".", exist_ok=True)
        _save_jsonl(normalized, save_jsonl_path, append=False)

    return normalized

# ------------------------------ EDIT NEWS(PER_STOCK) -----------------------------------------#
# --------------------------- finnhub ------------------------#
import os, json
from datetime import datetime, timedelta, timezone
import finnhub
import yfinance as yf


def _ensure_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def save_json(items: List[Dict], path: str):
    _ensure_dir(path)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for obj in items:
            f.write(json.dumps(obj, ensure_ascii=False))
            f.write("\n")

def _to_epoch(dt: datetime) -> int:
    return int(dt.replace(tzinfo=timezone.utc).timestamp())

def _window_epochs(curr_date: str, look_back_days: int) -> Tuple[int, int]:
    curr_dt  = datetime.strptime(curr_date, "%Y-%m-%d")
    start_dt = (curr_dt - timedelta(days=look_back_days)).replace(hour=0, minute=0, second=0, microsecond=0)
    end_dt   = curr_dt.replace(hour=23, minute=59, second=59, microsecond=0)
    return _to_epoch(start_dt), _to_epoch(end_dt)

def _epoch_to_iso(ts: int | float | None) -> Optional[str]:
    if isinstance(ts, (int, float)):
        return datetime.fromtimestamp(int(ts), tz=timezone.utc).isoformat()
    return None

def _norm_finnhub_item(symbol: str, it: Dict) -> Dict:
    ts = it.get("datetime")
    iso = _epoch_to_iso(ts)
    date_str = iso[:10] if iso else None
    return {
        "source": "finnhub",
        "symbol": symbol.upper(),
        "title": it.get("headline"),
        "summary": it.get("summary"),
        "publisher": it.get("source"),
        "url": it.get("url"),
        "published_epoch": int(ts) if isinstance(ts, (int, float)) else None,
        "published_iso": iso,
        "published_date": date_str,
        "raw": it,
        "_dedup_key": str(it.get("id") or it.get("url") or ""),
    }

def _norm_yf_item(symbol: str, it: Dict) -> Dict:
    ts = it.get("providerPublishTime")
    iso = _epoch_to_iso(ts)
    date_str = iso[:10] if iso else None
    # yfinance.Ticker(symbol).news: keys: title, link, publisher, providerPublishTime, type, uuid, ...
    return {
        "source": "yfinance",
        "symbol": symbol.upper(),
        "title": it.get("title"),
        "summary": None,  # YF ‡∏Ç‡πà‡∏≤‡∏ß‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ summary ‡∏ï‡∏£‡∏á‡πÜ
        "publisher": it.get("publisher"),
        "url": it.get("link"),
        "published_epoch": int(ts) if isinstance(ts, (int, float)) else None,
        "published_iso": iso,
        "published_date": date_str,
        "raw": it,
        "_dedup_key": str(it.get("uuid") or it.get("link") or ""),
    }


def fetch_company_news_finnhub(
    symbol: str,
    start_date: str,   # "YYYY-MM-DD"
    end_date: str,     # "YYYY-MM-DD"
    *,
    api_key: Optional[str] = None,
    client: Optional[finnhub.Client] = None,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó (Finnhub) ‡∏ï‡∏≤‡∏°‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô (‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏£‡∏µ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á~1‡∏õ‡∏µ)
    """
    if client is None:
        api_key = os.getenv("FINNHUB_API_KEY")
        if not api_key:
            raise RuntimeError("Missing FINNHUB_API_KEY")
        client = finnhub.Client(api_key=api_key)

    raw = client.company_news(symbol.upper(), _from=start_date, to=end_date) or []
    out = []
    seen = set()
    for it in raw:
        n = _norm_finnhub_item(symbol, it)
        if not n["_dedup_key"] or n["_dedup_key"] in seen:
            continue
        seen.add(n["_dedup_key"])
        # ‡∏Å‡∏±‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ timestamp
        if n["published_epoch"] is None:
            continue
        out.append(n)
    return out

# ---------------------------
# Merge & Orchestrator
# ---------------------------

def merge_company_news(
    *lists: Iterable[Dict],
    limit: int = 100
) -> List[Dict]:
    """
    ‡∏£‡∏ß‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á, ‡∏•‡∏ö‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ã‡πâ‡∏≥‡∏ï‡∏≤‡∏° _dedup_key, ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡∏î‡∏ï‡∏≤‡∏° limit
    """
    merged: List[Dict] = []
    seen = set()
    for lst in lists:
        for n in lst:
            dk = n.get("_dedup_key") or ""
            if not dk or dk in seen:
                continue
            seen.add(dk)
            merged.append(n)
    merged.sort(key=lambda x: (x.get("published_epoch") or 0), reverse=True)
    return merged[:limit]

def finnhub_get_company_news( symbol: str ) -> List[Dict]:
    """
    Orchestrator: ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≤‡∏Å Finnhub + yfinance ‚Üí ‡∏£‡∏ß‡∏°/‡∏•‡∏ö‡∏ã‡πâ‡∏≥/‡πÄ‡∏£‡∏µ‡∏¢‡∏á/‡∏ï‡∏±‡∏î
    - ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Ñ‡∏ß‡∏£‡∏™‡πà‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Finnhub (start_date/end_date)
    - ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏£‡∏≠‡∏á yfinance ‡∏î‡πâ‡∏ß‡∏¢ ‡∏Å‡πá‡∏™‡πà‡∏á curr_date + look_back_days ‡πÄ‡∏û‡∏¥‡πà‡∏°
    """
    # Finnhub window
    start_date: Optional[str] = (datetime.now(timezone.utc) - timedelta(days=30)).strftime("%Y-%m-%d")
    end_date: Optional[str] = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    # runtime
    limit: int = 50
    save_jsonl_path: Optional[str] = f"data/stock/{symbol}/finnhub_company_news.jsonl"
    items_fh: List[Dict] = []
    items_yf: List[Dict] = []
    finnhub_api_key = os.getenv("FINNHUB_API_KEY")

    if start_date and end_date:
        try:
            items_fh = fetch_company_news_finnhub(
                symbol, start_date, end_date, api_key=finnhub_api_key
            )
        except Exception:
            # ‡πÑ‡∏°‡πà‡∏•‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏á‡∏≤‡∏ô ‡∏´‡∏≤‡∏Å Finnhub ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
            items_fh = []

    merged = merge_company_news(items_fh, items_yf, limit=limit)
    reportmessage = f"üì∞ News :\n" \
                    f"Fetched {len(merged)} news items for {symbol} from Finnhub." 

    # write text file
    with open("all_report_message.txt", "a", encoding='utf-8') as file:
        file.write(reportmessage + "\n")

    # ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏™‡πà‡∏á save_jsonl_path ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏° {symbol} ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    if save_jsonl_path:
        try:
            path = save_jsonl_path.format(symbol=symbol)
        except Exception:
            path = save_jsonl_path  # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ placeholder ‡∏´‡∏£‡∏∑‡∏≠ format ‡∏ú‡∏¥‡∏î ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏á‡πÜ
        dirpath = os.path.dirname(path) or "."
        os.makedirs(dirpath, exist_ok=True)
        save_jsonl(merged, path, append=False)

    return merged

# ------------------------------ news stock subreddit -----------------------------------------#
from datetime import datetime, timedelta, timezone

REDDIT_TOKEN_URL  = "https://www.reddit.com/api/v1/access_token"
REDDIT_OAUTH_BASE = "https://oauth.reddit.com"

CLIENT_ID = os.getenv("REDDIT_ID")
CLIENT_SECRET = os.getenv("REDDIT_SECRET")
USER_AGENT = "news-fetcher:v1 (by u/keingkrai)"

# ---------- small io helpers ----------
def _ensure_parent_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def save_jsonl(items, path, append: bool = False):
    _ensure_parent_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

def _slug(s: str) -> str:
    """‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå/‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô"""
    if s is None:
        return "all"
    s = s.strip()
    s = re.sub(r"[\\/:*?\"<>|]+", "_", s)   # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏ö‡∏ô Windows
    s = re.sub(r"\s+", "_", s)              # ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô _
    return s[:120] or "all"                 # limit ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô

# ---------- reddit auth ----------
def get_token():
    s = requests.Session()
    s.headers.update({"User-Agent": USER_AGENT})
    auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)
    r = s.post(REDDIT_TOKEN_URL, auth=auth, data={"grant_type": "client_credentials"}, timeout=30)
    r.raise_for_status()
    return r.json()["access_token"]

# ---------- main search ----------
def reddit_get_company_news(query: str):
    """
    ‡∏Ñ‡πâ‡∏ô‡πÉ‡∏ô subreddit ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏î‡πâ‡∏ß‡∏¢ restrict_sr + sort=top (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢)
    - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á save_path ‚Üí ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: data\stock\<query>\reddit_search_<sub>_<YYYYMMDD>_<YYYYMMDD>.jsonl
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏°‡∏≠
    """
    sub: str = "news"
    start_dt: datetime = datetime.now(tz=timezone.utc) - timedelta(days=30)
    end_dt: datetime   = datetime.now(tz=timezone.utc)
    limit: int = 50
    save_path: str | None = None
    token = get_token()
    s = requests.Session()
    s.headers.update({
        "Authorization": f"bearer {token}",
        "User-Agent": USER_AGENT
    })

    # --------- ‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ----------
    if save_path is None:
        qslug = _slug(query)
        start_str = start_dt.astimezone(timezone.utc).strftime("%Y%m%d")
        end_str   = end_dt.astimezone(timezone.utc).strftime("%Y%m%d")
        save_path = f"data/stock/{qslug}/reddit_company_news_{end_str}.jsonl"

    url = f"{REDDIT_OAUTH_BASE}/r/{sub}/search.json"
    params = {
        "q": f"{query or ''}",
        "syntax": "cloudsearch",
        "restrict_sr": "true",
        "sort": "top",
        "limit": str(min(limit, 100)),
    }

    # ‡∏¢‡∏¥‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° retry ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏±‡∏ô 429/5xx
    for attempt in range(3):
        r = s.get(url, params=params, timeout=30)
        if r.status_code == 429:
            time.sleep(2 + attempt)
            continue
        r.raise_for_status()
        break

    out = []
    data = r.json().get("data", {}) or {}
    for ch in data.get("children", []) or []:
        d = ch.get("data", {}) or {}
        if d.get("over_18", False):
            continue
        out.append({
            "id": d.get("id"),
            "title": d.get("title", ""),
            "url": d.get("url", ""),
            "permalink": "https://reddit.com" + (d.get("permalink") or ""),
            "created_utc": d.get("created_utc", 0),
            "selftext": (d.get("selftext") or "").strip(),
            "flair": d.get("link_flair_text"),
            "subreddit": d.get("subreddit"),
            "score": d.get("score"),
            "num_comments": d.get("num_comments"),
        })
        time.sleep(0.05)

    # ‡πÄ‡∏ã‡∏ü (‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à)
    if save_path:
        save_jsonl(out, save_path, append=False)

    print(f"Fetched {len(out)} posts from r/{sub} for query='{query}' -> {save_path}")

    # write text file
    reportmessage = f"Fetched {len(out)} news items for {query} from Reddit."

    with open("all_report_message.txt", "a") as file:
        file.write(reportmessage + "\n")

    return out

# ------------------------------ news stock yfinance -----------------------------------------#
import yfinance as yf

def _ensure_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def _save_json(items: List[Dict], path: str):
    _ensure_dir(path)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

def _save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

def yfinance_get_company_news(symbol: str) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß '‡∏î‡∏¥‡∏ö' ‡∏à‡∏≤‡∏Å yfinance ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å API ‡∏ï‡∏£‡∏á ‡πÜ (‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤, ‡πÑ‡∏°‡πà‡∏•‡∏ö‡∏ã‡πâ‡∏≥, ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ü‡∏¥‡∏•‡∏î‡πå)
    ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô max_items (default 50)
    - ‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà symbol ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    - ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà ./data/stock/<symbol>/:
        - yfinance_company_news_<YYYY-MM-DD>.json
        - yfinance_company_news_<YYYY-MM-DD>.jsonl
    - ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö raw ‡∏ï‡∏≤‡∏° yfinance (‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô max_items)
    """
    max_items: int = 50
    t = yf.Ticker(symbol)
    try:
        news = t.get_news()            # ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏Ç‡∏≠‡∏á yfinance
    except Exception:
        news = getattr(t, "news", []) or []

    # coerce ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô list (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà)
    if not isinstance(news, list):
        try:
            news = list(news)
        except Exception:
            news = [news]

    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô max_items
    news = news[:max_items]

    # ‡πÄ‡∏ã‡∏ü‡πÅ‡∏ö‡∏ö raw ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ó‡∏±‡πâ‡∏á .json ‡πÅ‡∏•‡∏∞ .jsonl)
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    json_path = f"data/stock/{symbol}/yfinance_company_news_{today}.json"
    jsonl_path = f"data/stock/{symbol}/yfinance_company_news_{today}.jsonl"

    try:
        _save_json(news, json_path)
    except Exception:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ helper ‡∏ô‡∏µ‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ã‡∏ü‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°
        pass

    try:
        _save_jsonl(news, jsonl_path, append=False)
    except Exception:
        pass

    # write text file
    reportmessage = f"Fetched {len(news)} news items for {symbol} from YFinance."
    with open("all_report_message.txt", "a") as file:
        file.write(reportmessage + "\n")

    return news

# ------------------------------ Alpha vatantage news stock -----------------------------------------#
def alphavantage_get_company_news(
    symbol: str
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á '‡∏Ç‡πà‡∏≤‡∏ß‡∏î‡∏¥‡∏ö' ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏î‡πâ‡∏ß‡∏¢ Alpha Vantage NEWS_SENTIMENT
    - ‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà symbol (e.g., 'AAPL')
    - ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô max_items (default 50)
    - ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: ./data/stock/<symbol>/
        - alphavantage_company_news_<YYYY-MM-DD>.json
        - alphavantage_company_news_<YYYY-MM-DD>.jsonl
    - ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÅ‡∏ö‡∏ö raw ‡∏ï‡∏≤‡∏° Alpha Vantage (‡∏ñ‡∏π‡∏Å‡∏ï‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)
    ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:
    - ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ALPHAVANTAGE_API_KEY ‡πÉ‡∏ô env ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á api_key ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    - NEWS_SENTIMENT ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏ö‡∏ö time_from/time_to ‡πÉ‡∏ô‡∏£‡∏π‡∏õ YYYYMMDDTHHMM
    """
    look_back_days: int = 7
    max_items: int = 50
    base_url = "https://www.alphavantage.co/query"
    api_key = os.getenv("ALPHA_VANTAGE_API_KEY")
    if not api_key:
        raise ValueError("Missing ALPHAVANTAGE_API_KEY. Set env or pass api_key param.")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤ (UTC) ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á look_back_days ‡∏ß‡∏±‡∏ô
    end_dt = datetime.now(timezone.utc)
    start_dt = end_dt - timedelta(days=look_back_days)
    # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡∏≤‡∏° Alpha Vantage: YYYYMMDDTHHMM
    tf = start_dt.strftime("%Y%m%dT%H%M")
    tt = end_dt.strftime("%Y%m%dT%H%M")

    params = {
        "function": "NEWS_SENTIMENT",
        "tickers": symbol.upper(),
        "time_from": tf,
        "time_to": tt,
        "sort": "LATEST",        # ‡∏´‡∏£‡∏∑‡∏≠ "EARLIEST"
        "limit": str(max(10, min(max_items, 100))),  # AV ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ~100 ‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥‡∏Ç‡∏≠
        "apikey": api_key,
    }

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏û‡∏£‡πâ‡∏≠‡∏° retry ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡∏Å‡∏±‡∏ô‡πÄ‡∏Ñ‡∏™ 5xx/429
    session = requests.Session()
    items = []
    for attempt in range(3):
        try:
            r = session.get(base_url, params=params, timeout=30)
            if r.status_code in (429, 503):
                time.sleep(2 + attempt)
                continue
            r.raise_for_status()
            data = r.json() or {}

            # Alpha Vantage ‡∏≠‡∏≤‡∏à‡∏™‡πà‡∏á‡∏ü‡∏¥‡∏•‡∏î‡πå "Note" ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏¥‡∏î rate limit
            if "Note" in data:
                # ‡∏£‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
                time.sleep(5 + attempt * 2)
                continue

            # ‡∏Ç‡πà‡∏≤‡∏ß‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô key "feed"
            feed = data.get("feed", []) or []
            if not isinstance(feed, list):
                try:
                    feed = list(feed)
                except Exception:
                    feed = [feed]

            items = feed[:max_items]
            break
        except requests.HTTPError:
            if attempt == 2:
                raise
            time.sleep(2 + attempt)
        except Exception:
            if attempt == 2:
                raise
            time.sleep(1 + attempt)

    # ‡πÄ‡∏ã‡∏ü‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö raw
    today = end_dt.strftime("%Y-%m-%d")
    jsonl_path = f"data/stock/{symbol}/alphavantage_company_news_{today}.jsonl"

    try:
        _save_jsonl(items, jsonl_path, append=False)
    except Exception:
        pass

    # write text file
    reportmessage = f"Fetched {len(items)} news items for {symbol} from Alpha Vantage."
    with open("all_report_message.txt", "a") as file:
        file.write(reportmessage + "\n")

    return items

from GoogleNews import GoogleNews

def ryt9_get_company_news(symbol: str, limit: int = 15) -> list:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏∏‡πâ‡∏ô‡∏£‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏à‡∏≤‡∏Å Ryt9 ‡πÇ‡∏î‡∏¢‡∏ú‡πà‡∏≤‡∏ô Google News Filter
    symbol: ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô PTT, KBANK) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ .BK ‡∏Å‡πá‡πÑ‡∏î‡πâ
    """
    # ‡∏•‡∏ö .BK ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏Ñ‡πâ‡∏ô‡πÉ‡∏ô Google ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏µ‡∏¢‡∏ß‡πÜ
    clean_symbol = symbol.replace(".BK", "").upper()
    
    # Setup Google News (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢, ‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå‡πÑ‡∏ó‡∏¢)
    googlenews = GoogleNews(lang='th', region='TH')
    googlenews.set_period('7d') # ‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 7 ‡∏ß‡∏±‡∏ô
    
    # ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å site:ryt9.com
    # query ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: "PTT site:ryt9.com"
    query = f'"{clean_symbol}" site:ryt9.com'
    
    try:
        googlenews.search(query)
        results = googlenews.result()
        googlenews.clear()
        
        out = []
        for item in results:
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà (Google News ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô text ‡πÄ‡∏ä‡πà‡∏ô "2 hours ago" ‡∏´‡∏£‡∏∑‡∏≠ "Dec 18, 2025")
            # ‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô raw string ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡πá‡πÑ‡∏î‡πâ
            
            # ‡∏Å‡∏£‡∏≠‡∏á Title ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå
            title = item.get('title', '')
            
            out.append({
                "source": "ryt9 (via Google)",
                "symbol": f"{clean_symbol}.BK", # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏™‡πà .BK ‡∏Å‡∏•‡∏±‡∏ö‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á Format ‡∏£‡∏∞‡∏ö‡∏ö
                "title": title,
                "summary": item.get('desc'),
                "url": item.get('link'),
                "publisher": "Ryt9",
                "published_date": item.get('date'), # ‡πÄ‡∏õ‡πá‡∏ô String "x hours ago"
                "dedup_key": item.get('link')
            })
            
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö (Google News ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏∂‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏ö‡∏≤‡∏á‡∏ó‡∏µ‡∏Å‡πá‡πÑ‡∏°‡πà)
        return out[:limit]
        
    except Exception as e:
        print(f"‚ùå Error fetching Ryt9 for {symbol}: {e}")
        return []


# ------------------------------ EDIT SOCIAL MEDIA ---------------------------------#
# ------------------------------  BlueSky  ---------------------------------#
from atproto import Client, models as atp_models

# =============================== IO helpers ===============================

def _ensure_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for obj in items:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

# =============================== Bluesky core ==============================

def _post_url_from_uri(uri: str, handle: str) -> str:
    rkey = uri.split("/")[-1]
    return f"https://bsky.app/profile/{handle}/post/{rkey}"

def _collect_query(
    client: Client,
    q: str,
    limit_total: int,
    *,
    cursor: Optional[str] = None,
    sleep_s: float = 0.05,
    max_pages: int = 200,
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÅ‡∏ö‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤ (search_posts) ‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö limit_total ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏î‡∏´‡∏ô‡πâ‡∏≤
    ‡∏Ñ‡∏∑‡∏ô list ‡∏Ç‡∏≠‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà normalize ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß
    """
    out: List[Dict] = []
    pages = 0
    while len(out) < limit_total and pages < max_pages:
        params = atp_models.AppBskyFeedSearchPosts.Params(
            q=q,
            limit=min(100, limit_total - len(out)),
            cursor=cursor,
        )
        res = client.app.bsky.feed.search_posts(params=params)

        posts = res.posts or []
        for p in posts:
            author = p.author
            handle = getattr(author, "handle", "") or getattr(author, "did", "")
            display = getattr(author, "display_name", None) or handle
            record = p.record  # has .text, .created_at
            uri = p.uri
            out.append({
                "who": display,
                "handle": handle,
                "when": getattr(record, "created_at", None),
                "content": getattr(record, "text", "") or "",
                "url": _post_url_from_uri(uri, handle),
                "uri": uri,
                "source": "bsky",
            })
        cursor = getattr(res, "cursor", None)
        pages += 1
        if not cursor:
            break
        time.sleep(sleep_s)
    return out

# ============================ Public API (easy) ============================

def fetch_bsky_stock_posts(
    symbol: str
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå Bluesky ‡∏ó‡∏µ‡πà‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏´‡∏∏‡πâ‡∏ô/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå:
      - ‡∏Ñ‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 3 ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: <SYM>, $<SYM>, #<SYM> (‡πÄ‡∏ä‡πà‡∏ô NVDA, $NVDA, #NVDA)
      - ‡∏£‡∏ß‡∏°‡∏ú‡∏•, ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ uri, ‡πÉ‡∏™‡πà‡∏ü‡∏¥‡∏•‡∏î‡πå 'symbol', ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
      - ‡πÄ‡∏ã‡∏ü‡πÄ‡∏õ‡πá‡∏ô .jsonl ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö placeholder {symbol})

    Credentials:
      - ‡∏ï‡∏±‡πâ‡∏á ENV: BSKY_HANDLE, BSKY_APP_PW (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏™‡πà‡∏°‡∏≤‡∏ó‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡πá‡πÑ‡∏î‡πâ
    """
    limit_total: int = 50
    handle: Optional[str] = os.getenv("BSKY_HANDLE")
    app_password: Optional[str] = os.getenv("BSKY_APP_PW")
    service: Optional[str] = None
    save_jsonl_path: Optional[str] = f"data/social/{symbol}/bsky_{symbol}_posts.jsonl"
    # ---- ‡∏≠‡πà‡∏≤‡∏ô credentials ‡∏à‡∏≤‡∏Å env ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ----
    handle = handle or os.getenv("BSKY_HANDLE")
    app_password = app_password or os.getenv("BSKY_APP_PW")
    if not handle or not app_password:
        raise RuntimeError(
            "‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Bluesky credentials: BSKY_HANDLE ‡πÅ‡∏•‡∏∞ BSKY_APP_PW (App Password) "
            "‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ï‡πå handle=..., app_password=..."
        )

    # ---- login ----
    client = Client()
    if service:
        client.login(handle, app_password, service=service)
    else:
        client.login(handle, app_password)

    q_sym = symbol.upper()
    queries = [q_sym, f"${q_sym}", f"#{q_sym}"]

    # ‡∏ï‡πà‡∏≠ query ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö limit ‡∏£‡∏ß‡∏° (‡∏´‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤ ‡πÜ ‡∏Å‡∏±‡∏ô)
    per_query = max(1, limit_total // len(queries))
    all_rows: List[Dict] = []
    for q in queries:
        rows = _collect_query(client, q, per_query)
        all_rows.extend(rows)

    # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ uri
    seen = set()
    uniq: List[Dict] = []
    for r in all_rows:
        u = r.get("uri")
        if not u or u in seen:
            continue
        seen.add(u)
        r["symbol"] = q_sym
        uniq.append(r)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤ (ISO 8601 string) ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
    uniq.sort(key=lambda x: x.get("when") or "", reverse=True)

    # ‡∏ö‡∏µ‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô limit_total ‡∏à‡∏£‡∏¥‡∏á ‡πÜ
    if len(uniq) > limit_total:
        uniq = uniq[:limit_total]

    # ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå
    if save_jsonl_path:
        try:
            save_path = save_jsonl_path.format(symbol=q_sym)
        except Exception:
            save_path = save_jsonl_path
        save_jsonl(uniq, save_path, append=False)

    return uniq

# ------------------------------ mastodon --------------------------------#
from mastodon import Mastodon
from html import unescape
import re, json, os, time
from datetime import datetime
from urllib.parse import urljoin

def _ensure_dir(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for obj in items:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _strip_html(html: str) -> str:
    if not html:
        return ""
    text = re.sub(r"<br\s*/?>", "\n", html, flags=re.IGNORECASE)
    text = re.sub(r"<.*?>", "", text, flags=re.DOTALL)
    return unescape(text).strip()

def _to_iso(dt) -> str:
    try:
        return dt.isoformat()
    except Exception:
        return str(dt)

def _mk_client(instance_base_url: str, app_token: Optional[str]) -> Mastodon:
    if app_token:
        return Mastodon(access_token=app_token, api_base_url=instance_base_url)
    return Mastodon(api_base_url=instance_base_url)  # public read

def _timeline_hashtag_all(
    m: Mastodon, tag: str, *, limit_total: int, sleep_s: float = 0.05
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á timeline hashtag #<tag> ‡πÅ‡∏ö‡∏ö‡πÑ‡∏•‡πà‡∏´‡∏ô‡πâ‡∏≤ (max_id) ‡∏à‡∏ô‡∏Ñ‡∏£‡∏ö limit_total ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏°‡∏î
    """
    out: List[Dict] = []
    max_id = None
    while len(out) < limit_total:
        page = m.timeline_hashtag(tag, limit=min(40, limit_total - len(out)), max_id=max_id)
        if not page:
            break
        for t in page:
            out.append({
                "who": f'{t["account"].get("display_name") or t["account"].get("acct")}',
                "when": _to_iso(t.get("created_at")),
                "content": _strip_html(t.get("content", "")),
                "url": t.get("url") or t.get("uri"),
                "id": str(t.get("id")),
                "source": "mastodon#hashtag",
            })
        max_id = page[-1].get("id")
        time.sleep(sleep_s)
    return out

def _search_statuses(
    m: Mastodon, q: str, *, limit_total: int, sleep_s: float = 0.05
) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å search results (statuses) ‚Äî ‡πÉ‡∏ä‡πâ search_v2 ‡πÑ‡∏î‡πâ‡∏Å‡πá‡πÉ‡∏ä‡πâ, ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô fallback ‡πÄ‡∏õ‡πá‡∏ô search
    ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ö‡∏≤‡∏á instance ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå ‡∏Å‡πá‡∏à‡∏∞‡∏î‡∏∂‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà API ‡πÉ‡∏´‡πâ
    """
    try:
        if hasattr(m, "search_v2"):
            res = m.search_v2(q=q, type="statuses", limit=min(40, limit_total), resolve=True)
            statuses = res.get("statuses", []) if isinstance(res, dict) else []
        else:
            res = m.search(q=q, resolve=True, limit=min(40, limit_total))
            statuses = res.get("statuses", []) if isinstance(res, dict) else []
    except Exception:
        statuses = []

    out: List[Dict] = []
    for t in statuses:
        out.append({
            "who": f'{t["account"].get("display_name") or t["account"].get("acct")}',
            "when": _to_iso(t.get("created_at")),
            "content": _strip_html(t.get("content", "")),
            "url": t.get("url") or t.get("uri"),
            "id": str(t.get("id")),
            "source": "mastodon#search",
        })
    time.sleep(sleep_s)
    return out

def fetch_mastodon_stock_posts( symbol: str ) -> List[Dict]:
    """
    ‡∏î‡∏∂‡∏á‡πÇ‡∏û‡∏™‡∏ï‡πå Mastodon ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô:
      - Hashtag timeline: #<SYMBOL>
      - Text search: <SYMBOL>, $<SYMBOL>
    ‡∏£‡∏ß‡∏°, ‡∏•‡∏ö‡∏ã‡πâ‡∏≥ (id), ‡πÄ‡∏ï‡∏¥‡∏° 'symbol', ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å .jsonl ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

    ENV ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö:
      - MASTODON_BASE_URL (‡πÄ‡∏ä‡πà‡∏ô https://mastodon.social)
      - MASTODON_TOKEN    (app/user token ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤/‡∏Ç‡πâ‡∏≤‡∏°‡∏ö‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î)
    """
    # ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô/‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å ENV ‚Äî ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏™‡πà‡πÅ‡∏Ñ‡πà symbol ‡∏Å‡πá‡∏û‡∏≠
    instance_base_url: Optional[str] = None
    limit_hashtag: int = 120
    limit_search: int = 120
    app_token: Optional[str] = None
    save_jsonl_path: Optional[str] = f"data/social/{symbol}/mastodon_{symbol}_posts.jsonl"
    instance_base_url = instance_base_url or os.getenv("MASTODON_BASE_URL", "https://mastodon.social")
    app_token = app_token or os.getenv("MASTODON_TOKEN")

    m = _mk_client(instance_base_url, app_token)
    tag = symbol.upper()

    # 1) Hashtag timeline
    rows = _timeline_hashtag_all(m, tag, limit_total=limit_hashtag)

    # 2) Search statuses: SYMBOL ‡πÅ‡∏•‡∏∞ $SYMBOL
    rows += _search_statuses(m, tag,     limit_total=limit_search)
    rows += _search_statuses(m, f"${tag}", limit_total=limit_search)

    # de-dup by id
    seen = set()
    uniq: List[Dict] = []
    for r in rows:
        rid = r.get("id")
        if not rid or rid in seen:
            continue
        seen.add(rid)
        r["symbol"] = tag
        # ‡πÄ‡∏ï‡∏¥‡∏° absolute URL ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡∏ö‡∏≤‡∏á instance ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô /@user/‚Ä¶)
        if r.get("url") and r["url"].startswith("/"):
            r["url"] = urljoin(instance_base_url, r["url"])
        uniq.append(r)

    # sort latest first
    uniq.sort(key=lambda x: x.get("when") or "", reverse=True)

    # save
    if save_jsonl_path:
        try:
            out_path = save_jsonl_path.format(symbol=tag)
        except Exception:
            out_path = save_jsonl_path
        save_jsonl(uniq, out_path, append=False)

    return uniq

# ------------------------------ Subreddit social ---------------------------------#
import praw

DEFAULT_SUBS = [
    "stocks", "investing", "StockMarket", "wallstreetbets",
    "news", "business", "technology"
]

def _ensure_dir(path: str):
    d = os.path.dirname(path)
    if d:
        os.makedirs(d, exist_ok=True)

def save_jsonl(items: List[Dict], path: str, append: bool = False):
    _ensure_dir(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        for it in items:
            f.write(json.dumps(it, ensure_ascii=False) + "\n")

def _mk_reddit():
    return praw.Reddit(
        client_id=os.getenv("REDDIT_ID"),
        client_secret=os.getenv("REDDIT_SECRET"),
        user_agent="your-app:v1 (by u/yourname)",
        ratelimit_seconds=5,
    )

def fetch_reddit_symbol_top_praw( symbol: str ) -> List[Dict]:
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á symbol (‡πÄ‡∏ä‡πà‡∏ô NVDA, $NVDA) ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢ subreddit
    - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° top ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô timeframe ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    - ‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ post.id
    - ‡πÄ‡∏ã‡∏ü‡πÄ‡∏õ‡πá‡∏ô JSONL ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö {symbol},{timeframe} ‡πÉ‡∏ô‡∏û‡∏≤‡∏ò)
    """
    timeframe: str = "week"           # "hour","day","week","month","year","all"
    subs: Optional[List[str]] = None  # ‡∏ñ‡πâ‡∏≤ None ‡πÉ‡∏ä‡πâ DEFAULT_SUBS
    limit_per_sub: int = 50
    include_selftext: bool = False
    out_path: Optional[str] = f"data/social/{symbol}/reddit_{symbol}_post.jsonl"
    symbol_up = symbol.upper()
    query_variants = [symbol_up, f"${symbol_up}"]

    subs = subs or DEFAULT_SUBS
    reddit = _mk_reddit()
    results: List[Dict] = []
    seen = set()

    for sub in subs:
        sr = reddit.subreddit(sub)
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ query variant
        fetched = 0
        for q in query_variants:
            # ‡πÉ‡∏ä‡πâ search + sort=top + time_filter
            try:
                for p in sr.search(query=q, sort="top", time_filter=timeframe, limit=limit_per_sub):
                    if getattr(p, "over_18", False):
                        continue
                    if p.id in seen:
                        continue
                    seen.add(p.id)
                    item = {
                        "id": p.id,
                        "subreddit": sub,
                        "title": p.title,
                        "url": p.url,
                        "permalink": f"https://reddit.com{p.permalink}",
                        "created_utc": float(getattr(p, "created_utc", 0.0) or 0.0),
                        "score": getattr(p, "score", None),
                        "num_comments": getattr(p, "num_comments", None),
                        "flair": getattr(p, "link_flair_text", None),
                        "symbol": symbol_up,
                        "query": q,
                    }
                    if include_selftext:
                        item["selftext"] = (getattr(p, "selftext", "") or "").strip()
                    results.append(item)
                    fetched += 1
                    # ‡∏£‡∏∞‡∏ß‡∏±‡∏á rate-limit ‡∏ô‡∏¥‡∏î‡∏ô‡∏∂‡∏á
                    time.sleep(0.03)
            except Exception:
                # ‡∏ñ‡πâ‡∏≤‡∏ã‡∏±‡∏ö‡∏õ‡∏¥‡∏î/‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°
                continue

    # sort latest first (‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á)
    results.sort(key=lambda x: x.get("created_utc", 0.0), reverse=True)

    # save
    if out_path:
        try:
            out_path = out_path.format(symbol=symbol_up, timeframe=timeframe)
        except Exception:
            pass
        save_jsonl(results, out_path, append=False)

    return results
# ------------------------------ EDIT FUNDAMENTALS ---------------------------------#
# =========================
# CONFIGURATION
# =========================
FINNHUB_API_KEY      = os.getenv("FINNHUB_API_KEY")
ALPHAVANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API_KEY")
REQUEST_TIMEOUT      = float(os.getenv("REQ_TIMEOUT", "30"))

# Paths (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö {symbol})
DEFAULT_JSONL_PATH = r"data/fundamental/{symbol}/fundamentals_choice.jsonl"
DEFAULT_JSON_PATH  = r"data/fundamental/{symbol}/fundamentals_choice.json"
DEFAULT_RAW_JSON   = r"data/fundamental/{symbol}/fundamentals_raw.json"
REPORT_LOG_PATH    = "all_report_message.txt"

# Fields for comparison
NUM_FIELDS = {
    "overview":        ["marketCap", "sharesOutstanding", "peRatio"],
    "balancesheet":    ["totalAssets", "totalLiabilities", "shareholderEquity"],
    "cashflow":        ["operatingCashFlow", "freeCashFlow", "capitalExpenditures"],
    "incomestatement": ["totalRevenue", "netIncome", "eps"],
}
STR_FIELDS = {
    "overview": ["name", "currency", "exchange", "sector", "industry"],
}
PREFERRED_ORDER = ["yfinance", "finnhub", "alphavantage"]
UNIT_SCALES = [1, 10, 1000, 1_000_000, 1_000_000_000]

# =========================
# IO & UTILS
# =========================
def _ensure_dir_for(path: str):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

def _fmt_path(tpl: Optional[str], symbol: str) -> Optional[str]:
    if not tpl: return None
    try: p = tpl.format(symbol=symbol)
    except: p = tpl
    _ensure_dir_for(p)
    return p

def save_json(obj, path: str):
    _ensure_dir_for(path)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def save_jsonl_line(obj, path: str, append: bool = True):
    _ensure_dir_for(path)
    mode = "a" if append else "w"
    with open(path, mode, encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _try_float(x):
    try:
        if x in (None, "", "None", "NaN", "null"): return None
        return float(x)
    except: return None

def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def _match_units(a: float, b: float) -> bool:
    if a is None or b is None: return False
    for sa in UNIT_SCALES:
        for sb in UNIT_SCALES:
            # Check approx equal with tolerance
            if abs(a*sa - b*sb) <= max(1e-2, 1e-3 * max(abs(a*sa), abs(b*sb))):
                return True
    return False

def _str_equal(a: Optional[str], b: Optional[str]) -> bool:
    if not a or not b: return False
    return str(a).strip().lower() == str(b).strip().lower()

# =========================
# üß† SMART RESOLVE LOGIC
# =========================
def _is_valid_ticker(symbol: str) -> bool:
    """Check if ticker exists via yfinance fast_info (lightweight check)"""
    try:
        t = yf.Ticker(symbol)
        # Force a fetch of minimal data
        mc = t.fast_info.market_cap
        
        # If market_cap is None, it might be invalid or delisted. Check history to be sure.
        if mc is None:
             h = t.history(period="5d")
             return not h.empty

        return True
    except:
        # Fallback: Check if we can get history?
        try:
            h = t.history(period="5d")
            return not h.empty
        except:
            return False

def auto_resolve_symbol(symbol: str) -> str:
    """
    Auto-detect suffix:
    - "PTT" -> "PTT.BK" (If not found in US)
    - "600519" -> "600519.SS"
    - "GOLD"/"XAUUSD" -> "GOLD" (Barrick Gold for fundamentals)
    """
    s = symbol.upper().strip()
    
    # 0. Handle Gold/Commodities mapping to Company
    if s in ["XAUUSD", "GC=F", "GOLD_SPOT"]:
        return "GOLD" # Barrick Gold Corp

    # 1. If user already provided suffix, trust them
    if "." in s:
        return s

    # 2. Numeric Symbol (Likely China)
    if s.isdigit():
        if _is_valid_ticker(f"{s}.SS"): return f"{s}.SS"
        if _is_valid_ticker(f"{s}.SZ"): return f"{s}.SZ"
        # Fallback to HK (.HK) or just return raw
        return s

    # 3. Known Thai Stock Pattern (Common 3-6 chars, not standard US dict word often)
    # Heuristic: Try adding .BK FIRST if checking against Thai market is desired priority, 
    # OR try Thai .BK fallback aggressively.
    
    # Check explicitly if plain symbol works well in US
    # But many Thai stocks like 'CPALL' might not exist in US, so check US first.
    if _is_valid_ticker(s):
        # Even if valid in US, if it's a known Thai bluechip, we might prefer .BK? 
        # For now, stick to standard logic: US/Global first.
        return s
    
    # 3.2 Try Thai Suffix (.BK)
    if _is_valid_ticker(f"{s}.BK"):
        return f"{s}.BK"
    
    # 3.3 Default Fallback for potential Thai stocks not yet validated?
    # If it looks like a Thai ticker (up to 10 chars, typically 3-6), we can default to .BK
    # to let the fetchers try.
    # Ex: KBANK -> KBANK.BK (if KBANK US doesn't exist)
    return f"{s}.BK" if len(s) >= 3 else s

# =========================
# FETCHERS
# =========================

# --- YFinance ---
# --- YFinance ---
def _most_recent_col_frame(df) -> Optional[Tuple[str, Dict[str, float]]]:
    try:
        if df is None or getattr(df, "empty", True): return None
        # Ensure we have columns
        if len(df.columns) == 0: return None
        
        col0 = df.columns[0]
        series = df[col0]
        
        # Safe conversion
        data_dict = {}
        for k, v in series.items():
            try:
                data_dict[str(k)] = _try_float(v)
            except:
                pass
                
        return str(col0), data_dict
    except Exception as e:
        print(f"‚ö†Ô∏è YF DataFrame Parse Error: {e}")
        return None

def fetch_yfinance(symbol: str) -> Dict[str, Dict]:
    t = yf.Ticker(symbol)
    
    # Overview
    ov = {}
    try:
        fi = t.fast_info
        info = t.get_info()
        ov = {
            "marketCap": _try_float(getattr(fi, "market_cap", None)),
            "sharesOutstanding": _try_float(getattr(fi, "shares_outstanding", None)),
            "peRatio": _try_float(getattr(fi, "trailing_pe", None)),
            "currency": getattr(fi, "currency", None),
            "exchange": getattr(fi, "exchange", None),
            "name": info.get("shortName") or info.get("longName"),
            "sector": info.get("sector"),
            "industry": info.get("industry")
        }
    except: pass

    # Statements
    bs, cf, inc = {}, {}, {}
    
    # Balance Sheet
    try:
        # Ticker object might return differnt formats or timezone-aware checks failing
        bs_raw = _most_recent_col_frame(getattr(t, "balance_sheet", None))
        if bs_raw:
            r = bs_raw[1]
            bs = {
                "totalAssets": r.get("Total Assets"),
                "totalLiabilities": r.get("Total Liab") or r.get("Total Liabilities Net Minority Interest"),
                "shareholderEquity": r.get("Total Stockholder Equity") or r.get("Total Equity Gross Minority Interest")
            }
    except Exception as e: print(f"YF BS Error: {e}")

    # Cash Flow
    try:
        cf_raw = _most_recent_col_frame(getattr(t, "cashflow", None))
        if cf_raw:
            r = cf_raw[1]
            op = r.get("Total Cash From Operating Activities") or r.get("Operating Cash Flow")
            cap = r.get("Capital Expenditures")
            fcf = r.get("Free Cash Flow")
            if fcf is None and op is not None and cap is not None: fcf = op - cap
            cf = {"operatingCashFlow": op, "freeCashFlow": fcf, "capitalExpenditures": cap}
    except Exception as e: print(f"YF CF Error: {e}")

    # Income
    inc_raw = _most_recent_col_frame(getattr(t, "financials", None))
    if inc_raw:
        r = inc_raw[1]
        eps = None
        try: eps = _try_float(getattr(t.fast_info, "trailing_eps", None))
        except: pass
        inc = {"totalRevenue": r.get("Total Revenue"), "netIncome": r.get("Net Income"), "eps": eps}

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

# --- AlphaVantage ---
def fetch_alphavantage(symbol: str) -> Dict[str, Dict]:
    if not ALPHAVANTAGE_API_KEY: return {}
    
    def _get(func):
        try:
            url = "https://www.alphavantage.co/query"
            r = requests.get(url, params={"function": func, "symbol": symbol, "apikey": ALPHAVANTAGE_API_KEY}, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                d = r.json()
                if "Note" not in d and "Error Message" not in d: return d
        except: pass
        return {}

    ov_raw = _get("OVERVIEW")
    ov = {
        "name": ov_raw.get("Name"), "currency": ov_raw.get("Currency"),
        "exchange": ov_raw.get("Exchange"), "sector": ov_raw.get("Sector"),
        "marketCap": _try_float(ov_raw.get("MarketCapitalization")),
        "peRatio": _try_float(ov_raw.get("PERatio")),
        "sharesOutstanding": _try_float(ov_raw.get("SharesOutstanding"))
    }

    def _parse_rep(func, mapper):
        raw = _get(func)
        reports = raw.get("annualReports", [])
        if not reports: return {k: None for k in mapper}
        r = reports[0]
        return {k: _try_float(r.get(v)) for k, v in mapper.items()}

    bs = _parse_rep("BALANCE_SHEET", {"totalAssets": "totalAssets", "totalLiabilities": "totalLiabilities", "shareholderEquity": "totalShareholderEquity"})
    cf = _parse_rep("CASH_FLOW", {"operatingCashFlow": "operatingCashflow", "capitalExpenditures": "capitalExpenditures", "freeCashFlow": "freeCashFlow"}) # logic compute fcf separate if needed
    inc = _parse_rep("INCOME_STATEMENT", {"totalRevenue": "totalRevenue", "netIncome": "netIncome", "eps": "reportedEPS"})

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

# --- Finnhub ---
def fetch_finnhub(symbol: str) -> Dict[str, Dict]:
    if not FINNHUB_API_KEY: return {}
    # Optimization: Finnhub free doesn't support .BK/.SS well mostly.
    # But we try anyway or rely on clean symbol
    
    def _get(path, params):
        try:
            params["token"] = FINNHUB_API_KEY
            r = requests.get(f"https://finnhub.io/api/v1/{path}", params=params, timeout=REQUEST_TIMEOUT)
            return r.json() if r.status_code == 200 else {}
        except: return {}

    prof = _get("stock/profile2", {"symbol": symbol})
    ov = {
        "name": prof.get("name"), "currency": prof.get("currency"),
        "marketCap": _try_float(prof.get("marketCapitalization", 0)) * 1_000_000, # FH returns in Million usually
        "sharesOutstanding": _try_float(prof.get("shareOutstanding")),
        "sector": prof.get("finnhubIndustry")
    }
    
    # Financials
    rep = _get("stock/financials-reported", {"symbol": symbol})
    data = rep.get("data", [])
    bs, cf, inc = {}, {}, {}
    
    if data:
        report = data[0].get("report", {})
        
        def _find(lst, keys):
            for item in lst:
                if item.get("concept") in keys or item.get("label") in keys:
                    return _try_float(item.get("value"))
            return None

        # Simplified mapping (Real implementation needs comprehensive key mapping)
        bs = {
            "totalAssets": _find(report.get("bs", []), ["TotalAssets", "Assets"]),
            "totalLiabilities": _find(report.get("bs", []), ["Liabilities", "TotalLiabilities"]),
            "shareholderEquity": _find(report.get("bs", []), ["StockholdersEquity", "Equity"])
        }
        # ... (CF and INC mapping would be similar) ...

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

# =========================
# ORCHESTRATOR & SCORING
# =========================
async def fetch_all_fundamentals(symbol: str) -> Dict:
    # Use asyncio.gather to fetch data concurrently in separate threads
    # This prevents blocking the event loop while waiting for HTTP requests
    results = await asyncio.gather(
        asyncio.to_thread(fetch_yfinance, symbol),
        asyncio.to_thread(fetch_finnhub, symbol),
        asyncio.to_thread(fetch_alphavantage, symbol)
    )
    
    y, f, a = results

    # Log fields found
    def _count(d): return sum(1 for sec in d.values() for v in sec.values() if v is not None)
    
    log_msg = f"Fetched fundamentals for {symbol}: YF={_count(y)}, FH={_count(f)}, AV={_count(a)} fields."
    with open(REPORT_LOG_PATH, "a", encoding="utf-8") as file:
        file.write(log_msg + "\n")

    return {"symbol": symbol, "raw": {"yfinance": y, "finnhub": f, "alphavantage": a}}

def decide_single_source(fetched: Dict) -> Dict:
    raw = fetched["raw"]
    srcs = ["yfinance", "finnhub", "alphavantage"]
    scores = {s: 0 for s in srcs}
    completeness = {s: 0 for s in srcs}

    sections = ["overview", "balancesheet", "cashflow", "incomestatement"]
    
    for sec in sections:
        # Completeness
        for s in srcs:
            data = raw.get(s, {}).get(sec, {})
            cnt = sum(1 for v in data.values() if v is not None and v != "")
            completeness[s] += cnt
        
        # Cross-validation Score
        nums = NUM_FIELDS.get(sec, [])
        vals = {s: raw.get(s, {}).get(sec, {}) for s in srcs}
        
        # Compare pairs
        pairs = [("yfinance","finnhub"), ("yfinance","alphavantage"), ("finnhub","alphavantage")]
        for fld in nums:
            for s1, s2 in pairs:
                v1 = _try_float(vals[s1].get(fld))
                v2 = _try_float(vals[s2].get(fld))
                if _match_units(v1, v2):
                    scores[s1] += 1
                    scores[s2] += 1

    # Pick Winner
    top_score = max(scores.values())
    candidates = [s for s, sc in scores.items() if sc == top_score]
    
    winner = candidates[0]
    if len(candidates) > 1:
        # Tie-breaker 1: Completeness
        best_comp = max(completeness[s] for s in candidates)
        candidates_2 = [s for s in candidates if completeness[s] == best_comp]
        
        # Tie-breaker 2: Preference Order
        for p in PREFERRED_ORDER:
            if p in candidates_2:
                winner = p
                break

    return {
        "symbol": fetched["symbol"],
        "chosen_source": winner,
        "scores": scores,
        "completeness": completeness,
        "final_payload": raw.get(winner),
        "raw": raw,
        "timestamp": _now_iso()
    }

async def pick_fundamental_source(symbol: str) -> Dict:
    """
    Main Entry Point:
    1. Resolve Symbol (Add .BK, .SS, etc.)
    2. Fetch all sources
    3. Score & Pick winner
    4. Save Files
    """
    resolved_symbol = auto_resolve_symbol(symbol)
    print(f"Resolved to: {resolved_symbol}")

    fetched = await fetch_all_fundamentals(resolved_symbol)
    result = decide_single_source(fetched)
    # Save Logic
    # 1. Raw Data
    raw_path = _fmt_path(DEFAULT_RAW_JSON, resolved_symbol)
    if raw_path: save_json({"symbol": resolved_symbol, "raw": result["raw"]}, raw_path)

    # 2. Choice Data (Clean)
    json_path = _fmt_path(DEFAULT_JSON_PATH, resolved_symbol)
    if json_path: 
        clean_res = {k:v for k,v in result.items() if k != "raw"}
        save_json(clean_res, json_path)

    # 3. JSONL Append
    jsonl_path = _fmt_path(DEFAULT_JSONL_PATH, resolved_symbol)
    if jsonl_path: save_jsonl_line(result, jsonl_path)

    print(f"‚úÖ Data saved for {resolved_symbol}. Winner: {result['chosen_source']}")
    return result

# =========================
# 10-YEAR HISTORICAL LOGIC
# =========================

def _safe_date_str(d):
    try:
        if isinstance(d, (pd.Timestamp, datetime)):
            return d.strftime("%Y-%m-%d")
        return str(d)[:10]
    except: return str(d)

def fetch_yfinance_10y(symbol: str) -> Dict[str, Dict]:
    """Fetch all available historical data from YFinance (Exclude specific fields)"""
    print(f"   Running YFinance for {symbol}...")
    t = yf.Ticker(symbol)
    
    # 1. ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏≠‡∏≠‡∏Å (‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö key ‡∏Ç‡∏≠‡∏á yfinance ‡πÄ‡∏õ‡πä‡∏∞‡πÜ)
    EXCLUDE_KEYS = [
        "Interest Expense", 
        "Net Interest Income", 
        "Total Revenue"
    ]

    # Overview
    ov = {}
    try:
        info = t.get_info()
        fi = t.fast_info 
        ov = {
            "marketCap": _try_float(info.get("marketCap", getattr(fi, "market_cap", None))),
            "sharesOutstanding": _try_float(info.get("sharesOutstanding", getattr(fi, "shares_outstanding", None))),
            "peRatio": _try_float(info.get("trailingPE", getattr(fi, "trailing_pe", None))),
            "currency": info.get("currency", getattr(fi, "currency", None)),
            "name": info.get("shortName") or info.get("longName"),
            "sector": info.get("sector"),
            "industry": info.get("industry")
        }
    except: pass

    def _parse_df(df):
        out = {}
        if df is None or getattr(df, "empty", True): return out
        
        for col in df.columns:
            date_key = _safe_date_str(col)
            series_dict = df[col].to_dict()
            
            clean = {}
            for k, v in series_dict.items():
                key_str = str(k).strip() # ‡∏ï‡∏±‡∏î‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏á
                
                # 2. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ô‡∏µ‡πâ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ (Exclude List) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if key_str in EXCLUDE_KEYS:
                    continue # ‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏•‡∏¢ ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤

                val = _try_float(v)
                if val is not None:
                    clean[key_str] = val
            
            if clean:
                out[date_key] = clean
        return out

    try:
        bs = _parse_df(t.balance_sheet)
        cf = _parse_df(t.cashflow)
        inc = _parse_df(t.financials)
    except Exception as e:
        print(f"    YFinance Error: {e}")
        return {}

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

def fetch_alphavantage_10y(symbol: str) -> Dict[str, Dict]:
    if not ALPHAVANTAGE_API_KEY: return {}
    
    def _get(func):
        try:
            url = "https://www.alphavantage.co/query"
            r = requests.get(url, params={"function": func, "symbol": symbol, "apikey": ALPHAVANTAGE_API_KEY}, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                d = r.json()
                if "Note" not in d and "Error Message" not in d: return d
        except: pass
        return {}

    ov_raw = _get("OVERVIEW")
    ov = {
        "name": ov_raw.get("Name"), "currency": ov_raw.get("Currency"),
        "marketCap": _try_float(ov_raw.get("MarketCapitalization")),
        "peRatio": _try_float(ov_raw.get("PERatio"))
    }

    def _parse_reps(func, mapper):
        raw = _get(func)
        reports = raw.get("annualReports", [])
        out = {}
        for r in reports:
            # fiscalDateEnding is the key
            d = r.get("fiscalDateEnding")
            if not d: continue
            
            clean = {k: _try_float(r.get(v)) for k, v in mapper.items()}
            out[d] = clean
        return out

    bs = _parse_reps("BALANCE_SHEET", {"totalAssets": "totalAssets", "totalLiabilities": "totalLiabilities", "shareholderEquity": "totalShareholderEquity"})
    cf = _parse_reps("CASH_FLOW", {"operatingCashFlow": "operatingCashflow", "capitalExpenditures": "capitalExpenditures", "freeCashFlow": "freeCashFlow"}) # logic compute fcf separate if needed
    inc = _parse_reps("INCOME_STATEMENT", {"totalRevenue": "totalRevenue", "netIncome": "netIncome", "eps": "reportedEPS", "interestExpense": "interestExpense", "netInterestIncome": "netInterestIncome"})

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

def fetch_finnhub_10y(symbol: str) -> Dict[str, Dict]:
    if not FINNHUB_API_KEY: return {}
    
    def _get(path, params):
        try:
            params["token"] = FINNHUB_API_KEY
            r = requests.get(f"https://finnhub.io/api/v1/{path}", params=params, timeout=REQUEST_TIMEOUT)
            return r.json() if r.status_code == 200 else {}
        except: return {}

    prof = _get("stock/profile2", {"symbol": symbol})
    ov = {
        "name": prof.get("name"), "currency": prof.get("currency"),
        "marketCap": _try_float(prof.get("marketCapitalization", 0)) * 1_000_000
    }
    
    # Financials
    # freq='annual' for 10 years history usually
    rep = _get("stock/financials-reported", {"symbol": symbol, "freq": "annual"})
    data = rep.get("data", [])
    
    bs, cf, inc = {}, {}, {}
    
    def _find_val(lst, keys):
        for item in lst:
            if item.get("concept") in keys or item.get("label") in keys:
                return _try_float(item.get("value"))
        return None
        
    for entry in data:
        date_key = entry.get("endDate") or f"{entry.get('year')}-12-31" # fallback
        if not date_key: continue
        
        report = entry.get("report", {})
        
        # BS
        bs_clean = {
            "totalAssets": _find_val(report.get("bs", []), ["TotalAssets", "Assets"]),
            "totalLiabilities": _find_val(report.get("bs", []), ["Liabilities", "TotalLiabilities"]),
            "shareholderEquity": _find_val(report.get("bs", []), ["StockholdersEquity", "Equity"])
        }
        if any(v is not None for v in bs_clean.values()):
            bs[date_key] = bs_clean

        # CF
        cf_clean = {
            "operatingCashFlow": _find_val(report.get("cf", []), ["NetCashProvidedByUsedInOperatingActivities"]),
            "capitalExpenditures": _find_val(report.get("cf", []), ["PaymentsToAcquirePropertyPlantAndEquipment"]),    
        }
        if any(v is not None for v in cf_clean.values()):
            cf[date_key] = cf_clean

        # INC
        inc_clean = {
            "totalRevenue": _find_val(report.get("ic", []), ["Revenues", "SalesRevenueNet", "RevenuefromContractwithCustomerExcludingAssessedTax"]),
            "netIncome": _find_val(report.get("ic", []), ["NetIncomeLoss", "ProfitLoss"]),
            "interestExpense": _find_val(report.get("ic", []), ["InterestExpense", "InterestAndDebtExpense"]),
            "netInterestIncome": _find_val(report.get("ic", []), ["NetInterestIncome", "InterestIncomeExpenseNet"]),
        }
        if any(v is not None for v in inc_clean.values()):
            inc[date_key] = inc_clean

    return {"overview": ov, "balancesheet": bs, "cashflow": cf, "incomestatement": inc}

async def fetch_all_fundamentals_10y(symbol: str) -> Dict:
    results = await asyncio.gather(
        asyncio.to_thread(fetch_yfinance_10y, symbol),
        asyncio.to_thread(fetch_finnhub_10y, symbol),
        asyncio.to_thread(fetch_alphavantage_10y, symbol)
    )
    y, f, a = results
    return {"symbol": symbol, "raw": {"yfinance": y, "finnhub": f, "alphavantage": a}}

def decide_source_by_history(fetched: Dict, target_years=10) -> Dict:
    raw = fetched["raw"]
    srcs = ["yfinance", "finnhub", "alphavantage"]
    scores = {s: 0 for s in srcs}
    details = {s: {} for s in srcs}
    
    sections = ["balancesheet", "cashflow", "incomestatement"]
    
    for s in srcs:
        dataset = raw.get(s, {})
        # Count unique years across all sections
        years_seen = set()
        for sec in sections:
            sec_data = dataset.get(sec, {})
            # sec_data is Dict[DateStr, ValuesDict]
            for d in sec_data.keys():
                years_seen.add(d[:4]) # Just the year part
        
        count = len(years_seen)
        scores[s] = count
        details[s]["years_count"] = count
        details[s]["years"] = sorted(list(years_seen))
    
    # Pick Winner
    # 1. Max years
    max_years = max(scores.values())
    candidates = [s for s, sc in scores.items() if sc == max_years]
    
    print(candidates)
    
    winner = candidates[0]
    # Tie-breaker: Preference similar to original
    for p in PREFERRED_ORDER:
        if p in candidates:
            winner = p
            break
            
    return {
        "symbol": fetched["symbol"],
        "chosen_source": winner,
        "years_found": scores[winner],
        "available_years": details[winner]["years"],
        "all_scores": scores,
        "final_payload": raw.get(winner),
        "raw": raw,
        "timestamp": _now_iso()
    }

async def pick_fundamental_source_10years(symbol: str) -> Dict:
    resolved_symbol = auto_resolve_symbol(symbol)
    print(f"Resolved to: {resolved_symbol} (10 Years Mode)")

    fetched = await fetch_all_fundamentals_10y(resolved_symbol)
    result = decide_source_by_history(fetched)
    
    # Save
    _ensure_dir_for(f"data/fundamental/{resolved_symbol}/")
    save_json(result, f"data/fundamental/{resolved_symbol}/fundamentals_10y_choice.json")
    
    print(f"‚úÖ 10-Year Data saved for {resolved_symbol}. Winner: {result['chosen_source']} ({result['years_found']} years)")
    return result